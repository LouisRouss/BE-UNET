{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net and image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an introduction to image segmentation and more especially the U-Net neural network. It includes case studies with dataset available in the dataset repo aswell as the already trained models in the trained_model repo.\n",
    "\n",
    "0. [Requirements and import](#sec0)\n",
    "1. [Introduction to image segmentation](#sec1)\n",
    "2. [The U-Net network and its specifications](#sec2)\n",
    "3. [A simple case study](#sec3)\n",
    "4. [A case study on a more complex dataset](#sec4)\n",
    "5. [Conclusion](#sec5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec0\">0. Requirements and import</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be using `pytorch` and `torchvision`. We will also be using common libraries such as `numpy` or `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercise 1:\n",
    "Install the necessary packages and verify that everything is working by importing everything.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.utils\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "from functools import reduce\n",
    "from torch.autograd import Variable\n",
    "import itertools\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec1\">1. Introduction to image segmentation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image segmentation purpose is to label the pixels of a picture and thus recognize a certain amount of classes in it: the differents pixel having similar attributes are grouped. There are 2 possible types of image segmentation: semantic segmentation and instance segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Semantic Segmentation we label each pixel of an image with a corresponding class of what is being represented\n",
    "<img src=\"pictures/semantic_segmentation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instance Segmentation is one step ahead: each instance of a class is classified separatly\n",
    "\n",
    "<img src=\"pictures/instance_segmentation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of such tasks are not just labels or bounding box parameters but a high resolution image of usually the same size as the input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\">2. The U-Net network and its specifications</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net is a massively used neural network for image segmentation, the original paper introducing U-Net has been published in 2015. In this paper U-Net was used for a competition (ISBI challenge 2015 and ISBI challenge 2012) which goal was to track cells/neural structures on a dataset of medical images, it showed impressive results.\n",
    "<img src=\"pictures/unet_exemple.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually neural networks need a very large amount of data, in the medical area such dataset aren't always available. U-Net doesn't need that much data to be competitive, the use of transformation for data augmentation works perfectly fine. It also allows the network to learn invariance to these deformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the original structure of the network as introduced in the paper in 2015:\n",
    "<img src=\"pictures/unet_structure.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left side of the network (the \"contractive\" path or \"encoder\") allows the network to contextualize, the size of the image gradually reduces while the depth gradually increases: basically the networks learn the \"what\" information but loses the \"where\" information. The right side of the network (the \"extensive\" path or \"decoder\") allows the locate precisely the information, the size of the image gradually increases and the depth gradually decreases: the decoder recovers the \"where\" information by gradually upsampling. To get better location, at every step of the decoder we use skip connections by concatenating the output of the transposed convolution layers with the feature maps from the Encoder at the same level, a double convolution is then applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec3\">3. A simple case study</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part I used the code from : https://github.com/usuyama/pytorch-unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation and creation of data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate geometrical shapes and train the network to recognize these shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_data(height, width, count):\n",
    "    x, y = zip(*[generate_img_and_mask(height, width) for i in range(0, count)])\n",
    "\n",
    "    X = np.asarray(x) * 255\n",
    "    X = X.repeat(3, axis=1).transpose([0, 2, 3, 1]).astype(np.uint8)\n",
    "    Y = np.asarray(y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def generate_img_and_mask(height, width):\n",
    "    shape = (height, width)\n",
    "\n",
    "    triangle_location = get_random_location(*shape)\n",
    "    circle_location1 = get_random_location(*shape, zoom=0.7)\n",
    "    circle_location2 = get_random_location(*shape, zoom=0.5)\n",
    "    mesh_location = get_random_location(*shape)\n",
    "    square_location = get_random_location(*shape, zoom=0.8)\n",
    "    plus_location = get_random_location(*shape, zoom=1.2)\n",
    "\n",
    "    # Create input image\n",
    "    arr = np.zeros(shape, dtype=bool)\n",
    "    arr = add_triangle(arr, *triangle_location)\n",
    "    arr = add_circle(arr, *circle_location1)\n",
    "    arr = add_circle(arr, *circle_location2, fill=True)\n",
    "    arr = add_mesh_square(arr, *mesh_location)\n",
    "    arr = add_filled_square(arr, *square_location)\n",
    "    arr = add_plus(arr, *plus_location)\n",
    "    arr = np.reshape(arr, (1, height, width)).astype(np.float32)\n",
    "\n",
    "    # Create target masks\n",
    "    masks = np.asarray([\n",
    "        add_filled_square(np.zeros(shape, dtype=bool), *square_location),\n",
    "        add_circle(np.zeros(shape, dtype=bool), *circle_location2, fill=True),\n",
    "        add_triangle(np.zeros(shape, dtype=bool), *triangle_location),\n",
    "        add_circle(np.zeros(shape, dtype=bool), *circle_location1),\n",
    "        add_filled_square(np.zeros(shape, dtype=bool), *mesh_location),\n",
    "        add_plus(np.zeros(shape, dtype=bool), *plus_location)\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "    return arr, masks\n",
    "\n",
    "def add_square(arr, x, y, size):\n",
    "    s = int(size / 2)\n",
    "    arr[x-s,y-s:y+s] = True\n",
    "    arr[x+s,y-s:y+s] = True\n",
    "    arr[x-s:x+s,y-s] = True\n",
    "    arr[x-s:x+s,y+s] = True\n",
    "\n",
    "    return arr\n",
    "\n",
    "def add_filled_square(arr, x, y, size):\n",
    "    s = int(size / 2)\n",
    "\n",
    "    xx, yy = np.mgrid[:arr.shape[0], :arr.shape[1]]\n",
    "\n",
    "    return np.logical_or(arr, logical_and([xx > x - s, xx < x + s, yy > y - s, yy < y + s]))\n",
    "\n",
    "def logical_and(arrays):\n",
    "    new_array = np.ones(arrays[0].shape, dtype=bool)\n",
    "    for a in arrays:\n",
    "        new_array = np.logical_and(new_array, a)\n",
    "\n",
    "    return new_array\n",
    "\n",
    "def add_mesh_square(arr, x, y, size):\n",
    "    s = int(size / 2)\n",
    "\n",
    "    xx, yy = np.mgrid[:arr.shape[0], :arr.shape[1]]\n",
    "\n",
    "    return np.logical_or(arr, logical_and([xx > x - s, xx < x + s, xx % 2 == 1, yy > y - s, yy < y + s, yy % 2 == 1]))\n",
    "\n",
    "def add_triangle(arr, x, y, size):\n",
    "    s = int(size / 2)\n",
    "\n",
    "    triangle = np.tril(np.ones((size, size), dtype=bool))\n",
    "\n",
    "    arr[x-s:x-s+triangle.shape[0],y-s:y-s+triangle.shape[1]] = triangle\n",
    "\n",
    "    return arr\n",
    "\n",
    "def add_circle(arr, x, y, size, fill=False):\n",
    "    xx, yy = np.mgrid[:arr.shape[0], :arr.shape[1]]\n",
    "    circle = np.sqrt((xx - x) ** 2 + (yy - y) ** 2)\n",
    "    new_arr = np.logical_or(arr, np.logical_and(circle < size, circle >= size * 0.7 if not fill else True))\n",
    "\n",
    "    return new_arr\n",
    "\n",
    "def add_plus(arr, x, y, size):\n",
    "    s = int(size / 2)\n",
    "    arr[x-1:x+1,y-s:y+s] = True\n",
    "    arr[x-s:x+s,y-1:y+1] = True\n",
    "\n",
    "    return arr\n",
    "\n",
    "def get_random_location(width, height, zoom=1.0):\n",
    "    x = int(width * random.uniform(0.1, 0.9))\n",
    "    y = int(height * random.uniform(0.1, 0.9))\n",
    "\n",
    "    size = int(min(width, height) * random.uniform(0.06, 0.12) * zoom)\n",
    "\n",
    "    return (x, y, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_array(img_array, ncol=3):\n",
    "    nrow = len(img_array) // ncol\n",
    "\n",
    "    f, plots = plt.subplots(nrow, ncol, sharex='all', sharey='all', figsize=(ncol * 4, nrow * 4))\n",
    "\n",
    "    for i in range(len(img_array)):\n",
    "        plots[i // ncol, i % ncol]\n",
    "        plots[i // ncol, i % ncol].imshow(img_array[i])\n",
    "\n",
    "def plot_side_by_side(img_arrays):\n",
    "# Allows to plot a set of images and their masks side by side\n",
    "    flatten_list = reduce(lambda x,y: x+y, zip(*img_arrays))\n",
    "\n",
    "    plot_img_array(np.array(flatten_list), ncol=len(img_arrays))\n",
    "\n",
    "def plot_errors(results_dict, title):\n",
    "    markers = itertools.cycle(('+', 'x', 'o'))\n",
    "\n",
    "    plt.title('{}'.format(title))\n",
    "\n",
    "    for label, result in sorted(results_dict.items()):\n",
    "        plt.plot(result, marker=next(markers), label=label)\n",
    "        plt.ylabel('dice_coef')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc=3, bbox_to_anchor=(1, 0))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def masks_to_colorimg(masks):\n",
    "# Allows to map each different classes (geometrical shapes) to each color\n",
    "    colors = np.asarray([(201, 58, 64), (242, 207, 1), (0, 152, 75), (101, 172, 228),(56, 34, 132), (160, 194, 56)])\n",
    "\n",
    "    colorimg = np.ones((masks.shape[1], masks.shape[2], 3), dtype=np.float32) * 255\n",
    "    channels, height, width = masks.shape\n",
    "\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            selected_colors = colors[masks[:,y,x] > 0.5]\n",
    "\n",
    "            if len(selected_colors) > 0:\n",
    "                colorimg[y,x,:] = np.mean(selected_colors, axis=0)\n",
    "\n",
    "    return colorimg.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 192, 192, 3)\n",
      "0 255\n",
      "(3, 6, 192, 192)\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Generate some random images\n",
    "input_images, target_masks = generate_random_data(192, 192, count=3)\n",
    "\n",
    "for x in [input_images, target_masks]:\n",
    "    print(x.shape)\n",
    "    print(x.min(), x.max())\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [masks_to_colorimg(x) for x in target_masks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercise 2:\n",
    "Using plot_side_by_side(), plot the different images and masks previously created.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ex2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now generate the dataset and create the data loader. Here we don't need any specific transformation or deformation for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 2000, 'val': 200}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count, transform=None):\n",
    "        self.input_images, self.target_masks = generate_random_data(192, 192, count=count)        \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return [image, mask]\n",
    "\n",
    "# use same transform for train/val for this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_set = SimDataset(2000, transform = trans)\n",
    "val_set = SimDataset(200, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "}\n",
    "\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 3, 192, 192]) torch.Size([25, 6, 192, 192])\n",
      "0.0 1.0 0.022927517 0.14967239\n",
      "0.0 1.0 0.004650065 0.06803273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1b8f4572e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reverse_transform(inp):\n",
    "#from tensor to numpy array\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "    \n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "for x in [inputs.numpy(), masks.numpy()]:\n",
    "    print(x.min(), x.max(), x.mean(), x.std())\n",
    "\n",
    "plt.imshow(reverse_transform(inputs[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the U-Net model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercise 3:\n",
    "Fill the void using the U-Net network structure previously seen. The network here goes a little less deep than the one from the paper.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-6c49385ae2ec>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-6c49385ae2ec>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    self.dconv_down1 = double_conv(3, ?? )\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )   \n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(3, ?? )\n",
    "        self.dconv_down2 = double_conv(??, ??)\n",
    "        self.dconv_down3 = double_conv(??, ??)\n",
    "        self.dconv_down4 = double_conv(??, ??)        \n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "        \n",
    "        self.dconv_up3 = double_conv(?? + 512, 256)\n",
    "        self.dconv_up2 = double_conv(?? + 256, 128)\n",
    "        self.dconv_up1 = double_conv(?? + 64, 64)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)   \n",
    "        \n",
    "        x = self.dconv_down4(x)\n",
    "        \n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        \n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv2], dim=1)       \n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)   \n",
    "        \n",
    "        x = self.dconv_up1(x)\n",
    "        \n",
    "        out = self.conv_last(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ex3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (dconv_down1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (dconv_down2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (dconv_down3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (dconv_down4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "  (dconv_up3): Sequential(\n",
      "    (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (dconv_up2): Sequential(\n",
      "    (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (dconv_up1): Sequential(\n",
      "    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv_last): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# we are using there if available, the GPU acceleration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = UNet(6)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define here the different loss functions, the loss used during the training for backpropragation is a balance between the Dice loss and the BCE with logits loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    \n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    \n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "        \n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)             \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### don't launch the training unless you got ~1h30min to lose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 6\n",
    "\n",
    "model = UNet(num_class).to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=0.1)\n",
    "\n",
    "# Uncomment to launch training\n",
    "# model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pre trained model instead ! If the previous cell printed cpu don't run the 2 next cells, you can see the result directly on the picture, if it printed 0 go ahead !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = F\"trained_model/Unet_geomform.pt\"\n",
    "# model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our models perform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()   # Set model to evaluate mode\n",
    "\n",
    "# test_dataset = SimDataset(10, transform = trans)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=0)\n",
    "        \n",
    "# inputs, labels = next(iter(test_loader))\n",
    "# inputs = inputs.to(device)\n",
    "# labels = labels.to(device)\n",
    "\n",
    "# pred = model(inputs)\n",
    "\n",
    "# pred = pred.data.cpu().numpy()\n",
    "# print(pred.shape)\n",
    "\n",
    "# # Change channel-order and make 3 channels for matplot\n",
    "# input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n",
    "\n",
    "# # Map each channel (i.e. class) to each color\n",
    "# target_masks_rgb = [masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "# pred_rgb = [masks_to_colorimg(x) for x in pred]\n",
    "\n",
    "# plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is the result if you can't run the previous cells. On the left the input image, in the center the target mask and on the right the prediction \n",
    "<img src=\"pictures/result1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good isn't it ? We can see that most of the geometrical shapes are well recognized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec4\">4. A case study on a more complex dataset</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometrical shapes are good but let's now work on a more complex dataset. We will see the use of Unet directly on the dataset from the ISBI challenge 2012 !\n",
    "Let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISBI2012Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path_img, path_target, transforms=None):\n",
    "        self.train = np.expand_dims(tifffile.TiffFile(path_img).asarray(), axis=-1)\n",
    "        self.targets = np.expand_dims(tifffile.TiffFile(path_target).asarray(), axis=-1)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.train[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "            target = self.transforms(target)\n",
    "            \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "isbi = ISBI2012Dataset('./dataset/train-volume.tif', './dataset/train-labels.tif',\n",
    "                       transforms=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 512, 512, 1)\n",
      "(30, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "print(isbi.train.shape)\n",
    "print(isbi.targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercise 4:\n",
    "print the images, take a look at the data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ex4.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ex4.2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the U-Net model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is here almost the same, we just add some BatchNorm2D and change the in channel size for the first conv and the number of classes detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Conv, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.batch_norm(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.batch_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet2, self).__init__()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.up4 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        self.conv1 = Conv(1, 32)\n",
    "        self.conv2 = Conv(32, 64)\n",
    "        self.conv3 = Conv(64, 128)\n",
    "        self.conv4 = Conv(128, 256)\n",
    "        \n",
    "        self.conv5 = Conv(256, 512)\n",
    "        \n",
    "        self.conv6 = Conv(768, 256)\n",
    "        self.conv7 = Conv(384, 128)\n",
    "        self.conv8 = Conv(192, 64)\n",
    "        self.conv9 = Conv(96, 32)\n",
    "        \n",
    "        self.conv10 = nn.Conv2d(32, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1(x)\n",
    "        x = self.pool1(c1)\n",
    "        c2 = self.conv2(x)\n",
    "        x = self.pool2(c2)\n",
    "        c3 = self.conv3(x)\n",
    "        x = self.pool3(c3)\n",
    "        c4 = self.conv4(x)\n",
    "        x = self.pool4(c4)\n",
    "        x = self.conv5(x)\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat([x, c4], 1)\n",
    "        x = self.conv6(x)\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, c3], 1)\n",
    "        x = self.conv7(x)\n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, c2], 1)\n",
    "        x = self.conv8(x)\n",
    "        x = self.up4(x)\n",
    "        x = torch.cat([x, c1], 1)\n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet2(\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (up1): Upsample(scale_factor=2.0, mode=nearest)\n",
      "  (up2): Upsample(scale_factor=2.0, mode=nearest)\n",
      "  (up3): Upsample(scale_factor=2.0, mode=nearest)\n",
      "  (up4): Upsample(scale_factor=2.0, mode=nearest)\n",
      "  (conv1): Conv(\n",
      "    (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conv2): Conv(\n",
      "    (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conv3): Conv(\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conv4): Conv(\n",
      "    (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conv5): Conv(\n",
      "    (batch_norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conv6): Conv(\n",
      "    (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conv7): Conv(\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conv8): Conv(\n",
      "    (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conv9): Conv(\n",
      "    (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conv10): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = UNet2()\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We once again define the trainer (we eventually could've adapted the previously used trainer). We here use only the BCE with logits loss criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit_generator(self, dataset, criterion, optimizer, n_epochs=1, batch_size=1, shuffle=False):\n",
    "        loss_history = []\n",
    "                \n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            print('Epoch : {}/{}'.format(epoch + 1, n_epochs))\n",
    "            print('-'*10)\n",
    "            \n",
    "            for batch, (data, target) in enumerate(loader):\n",
    "                data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = criterion(output, target)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                if (batch % 10) == 9:\n",
    "                    print('\\tBatch : {}/{}\\tLoss : {:.4f}'.format(batch+1, len(loader), loss.item()))\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            loss_history.append(running_loss/len(loader))\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x)\n",
    "        x = Variable(x.unsqueeze(0).cuda())\n",
    "        output = F.sigmoid(self.model(x)).data.cpu()\n",
    "        return output.numpy()\n",
    "\n",
    "    def predict_generator(self, dataset, batch_size=1):\n",
    "        predictions = []\n",
    "        loader = DataLoader(dataset, batch_size=batch_size)\n",
    "        for batch, (data, target) in enumerate(loader):\n",
    "            data = Variable(data.cuda())\n",
    "            outputs = self.model(data)\n",
    "            for prediction in outputs:\n",
    "                predictions.append(F.sigmoid(prediction).data.cpu().numpy())\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't launch the training unless you got ~30min to lose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Uncomment to launch training\n",
    "# loss_history = trainer.fit_generator(isbi, criterion, optimizer, 25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pre trained model instead ! If the previous cell printed cpu don't run the commented cells, you can see the result directly on the different pictures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = F\"trained_model/UNet_originalDataSet.pt\" \n",
    "# model.load_state_dict(torch.load(path,map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the loss history that we obtain after training\n",
    "<img src=\"pictures/loss_history.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img, target = isbi[0]\n",
    "#y_pred = trainer.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.9601518 , 0.9820613 , 0.98728603, ..., 0.99326986,\n",
       "          0.9900733 , 0.9752906 ],\n",
       "         [0.98018205, 0.9909508 , 0.99324495, ..., 0.9953069 ,\n",
       "          0.9944476 , 0.989369  ],\n",
       "         [0.9853477 , 0.9936807 , 0.99524105, ..., 0.9965166 ,\n",
       "          0.9956735 , 0.993378  ],\n",
       "         ...,\n",
       "         [0.9904108 , 0.9965064 , 0.99744546, ..., 0.9959915 ,\n",
       "          0.9936532 , 0.9911857 ],\n",
       "         [0.98589027, 0.9942198 , 0.9959027 , ..., 0.9945304 ,\n",
       "          0.9925097 , 0.9852701 ],\n",
       "         [0.9658562 , 0.9867771 , 0.989655  , ..., 0.98986965,\n",
       "          0.98605806, 0.9708689 ]]]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lt.imshow(y_pred.reshape(512, 512), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/figure2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks good but it's a little blurry, let's fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh = 0.5\n",
    "# y_pred[y_pred >= thresh] = 1\n",
    "# y_pred[y_pred < thresh] = 0\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(y_pred.reshape(512, 512), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/pic3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(target.numpy().reshape(512, 512), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/figure4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec5\">5. Conclusion</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seeb the application of two simple U-Net networks on two different datasets. Different things could be improved. On the first dataset we could change the data generator so we got a random number of random geometrical shapes instead of always the 6 same shapes. It would add some more challenge for the model. \n",
    "Secondly we haven't used data augmentation for the second dataset, it would probably lead to a better result but the training time would be also widely increased"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
